{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSE 200 FINAL PROJECT\n",
    "## Fall 2018\n",
    "### Due Date:  December 7th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is comprised of two parts: \n",
    "* <b>Part I</b> is a set of coding questions that require the _numpy_ library to analyze the provided dataset.  \n",
    "* <b>Part II</b> is a guided project for you to build your own end-to-end analysis using Python, especially using what you learned on Python _IO_, _pandas_, _matplotlib_ and _scilitlearn_ libraries.  \n",
    "\n",
    "<b>Deliverables</b>: Submit both parts as one notebook via Github by midnight on the due date above along with clear instructions on how to download the datasets you used for Part II and reproduce your results. The notebook should be organized with a clear table of contents on top _(see example in the Pylaski notebook from Day 5)_ and links to the parts/steps outlined. Don't forget to add your name on top as the author of the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preliminaries\n",
    "\n",
    "Use numpy to load `iris.npy` into a numpy matrix. Print the dataset's shape and the first 5 rows.<br>\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Tuple representing dataset's shape</li>\n",
    "    <li>Matrix representing the first 5 rows</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# For use later\n",
    "column_names = ['Id','SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Species']\n",
    "species_encoding = {'Iris-setosa': 1, 'Iris-versicolor': 2, 'Iris-virginica': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Transformations\n",
    "\n",
    "The first column is the id of the sample, which isn't relevant for our purposes. Remove that column from the matrix by creating a new matrix composed of the rest of the columns.<br>\n",
    "As usual, print the shape of the resulting dataset and the first 5 rows.\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Tuple representing dataset's shape</li>\n",
    "    <li>Matrix representing the first 5 rows</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Summary Statistics\n",
    "\n",
    "*Note: Don't worry about the order in which you display the values in this section. Display them in whatever order/grouping makes most sense to you*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Print the means and standard deviations of each column.\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Floats representing the standard deviation of each column</li>\n",
    "    <li>Floats representing the mean of each column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Print the minimum and maximum values of each column\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Floats representing the minimum value found in each column</li>\n",
    "    <li>Floats representing the maximum value found in each column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Calculate the species-wise means and standard deviations.<br>\n",
    "**Report these values with respect to the actual *name* of the species, for which you must refer to 1.1**\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>For each of the 3 species in the dataset:<ul>\n",
    "        <li>Floats representing the standard deviation of each column for this species</li>\n",
    "        <li>Floats representing the mean of each column for this species</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Advanced list comprehensions and numpy\n",
    "\n",
    "Use **list comprehensions** to generate a list of tuples for each species.<br>\n",
    "For a given species, the list will represent columns and their mean values. So, each tuple will be of the form `(column_name, column_mean)` and you'll have one per column. You can check your intuition using your **1.3c** output <br>\n",
    "Note that the column names are listed in **1.1** and recall that you dropped the id column.<br><br>\n",
    "Each list will have the following format:\n",
    "    `[(column_name, column_mean), (column_name, column_mean), ...]`\n",
    "    \n",
    "   *hint*: The enumerate function might be helpful in creating a concise comprehension<br>\n",
    "\n",
    "**Output required**: \n",
    "<ul>\n",
    "    <li>Three lists of tuples. One list per species, each list has a tuple per column</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II : 80%\n",
    "\n",
    "This project is culmination of all you’ve learned in this course! You should expect to spend <b>24-32 total hours</b> on the project. Be sure to read all of the items below before starting.\n",
    "\n",
    "There are a number of steps outlined below, but is critical that you do not view this as an entirely linear process.  Remember that the science component in data science is the creation of a hypothesis based on exploration and testing of that hypothesis through analysis.  You may need to go through many of these steps multiple times before you arrive at meaningful hypothesis or conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Find a dataset or datasets\n",
    "\n",
    "Based on your interest, identify a dataset which you will want to examine.  You will find a starting point for where you can find open datasets at the end of this notebook, but feel free to use other datasets you have access to and can publicly share results about. \n",
    " \n",
    "\n",
    "This step may take some time, as you’ll likely look at a number of datasets before you find one (or more) which holds promising data for the kinds of questions you want to ask. You are expected to use at least two interconnected datasets, e.g., two tables in one database or a combination of datasets which you can merge in some meaningful way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the datasets\n",
    "\n",
    "In this step, you should explore what is present in the data and how the data are organized. You’ll need to determine what common features allow you to merge the datasets.  \n",
    "\n",
    "You are expected to answer the following questions using the _pandas_ library and markdown cells to describe your actions:\n",
    "\n",
    "* Are there quality issues in the dataset (noisy, missing data, etc.)? \n",
    "* What will you need to do to clean and/or transform the raw data for analysis?\n",
    "\n",
    "You are also expected to use the _matplotlib_ library to visually explore the datasets and explain your findings, specifically,\n",
    "\n",
    "* How are the data distributed? \n",
    "* What are some common trends?\n",
    "* What are the relationships between variables in your datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Identify 1-3 research questions and perform analysis\n",
    "\n",
    "Now that you have a better understanding of the data, you will want to form a research question which is interesting to you. The research question should be broad enough to be of interest to a reader but narrow enough that the question can be answered with the data.  Some examples:\n",
    "\n",
    "* __Too Narrow:__  What is the GDP of the U.S. for 2011?  This is just asking for a fact or a single data point.  \n",
    "\n",
    "* __Too Broad:__  What is the primary reason for global poverty?  This could be a Ph.D. thesis and would still be way too broad.  What data will you use to answer this question?  Even if a single dataset offered an answer, would it be defendable given the variety of datasets out there?\n",
    "\n",
    "* __Good:__  Can you use simple sentiment analysis on comments about movies in a movie database to predict its box office earnings?  If you have, or can obtain, data on a variety of movies and you have their box office earnings, this is a question which you can potentially answer well. \n",
    "\n",
    "__Remember__, this course is for learning Python. You will not be graded on the complexity, accuracy or performance of your analytical methods. However, you are expected to use a Python library, e.g., _scikitlearn_, successfully to generate results and explain why you picked the methods you used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Present your findings\n",
    "\n",
    "In this step, you can begin to report your findings.  What did you learn from the data and how do your findings help answer your research question?  Use _matplotlib_ visualizations to present these findings.\n",
    "\n",
    "\n",
    "__Remember:__ Rarely will a single data analysis conclusively answer a research question.  Here, you need to identify possible limitations.  For example, are your results limited to a certain area, city, or country?  Are you making assumptions about the data which may, or may not, be valid (e.g., that students in one term are equally qualified as students in another)?  Document these limitations in a few paragraphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to find project datasets?\n",
    "\n",
    "There are a surprising number of open datasets on the web.  There are also a number of excellent websites which help keep track of where to find datasets.  Rather than attempt to recreate the good work from these teams, here's where you can find there collections:\n",
    "\n",
    "* UCI's Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets.html) - UC Irvine maintains a fantastic collection of datasets for machine learning, tagged by machine learning task (among other things).  \n",
    "* Kaggle (https://www.kaggle.com/datasets) - Online collection of datasets.  Kaggle also has competitions for data mining and information about jobs in data science.\n",
    "* KDnuggets Dataset (http://www.kdnuggets.com/datasets/index.html) - Another collection of datasets - most of the datasets are free. Within KDnuggets, there are links for government data (http://www.kdnuggets.com/datasets/government-local-public.html), Data APIs (http://www.kdnuggets.com/datasets/api-hub-marketplace-platform.html),  and Data Mining Competitions (http://www.kdnuggets.com/competitions/index.html).\n",
    "* There are also many sites that share open government and organizational datasets: US Government Data (https://www.data.gov/), UK Government Data (https://data.gov.uk/), Canada's Open Data Exchange (https://codx.ca/), World Health Organization (http://www.who.int/gho/en/), and the World Bank (http://data.worldbank.org/). \n",
    "\n",
    "Remember, finding a dataset of interest and exploring it is most of your job for your final project. So expect this to take some time - and that's both perfectly normal and completely okay. Get started early and make sure to check if the dataset you find satisfies the project requirements.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
